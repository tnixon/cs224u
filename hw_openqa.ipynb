{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf85bea0-68bf-4405-96ec-37579b2e9587",
   "metadata": {},
   "source": [
    "# Homework and bakeoff: Few-shot OpenQA with DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e9bf5-7956-4c63-9129-7f2cbc468075",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts and Omar Khattab\"\n",
    "__version__ = \"CS224u, Stanford, Fall 2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5d964-a45c-496a-bb46-8f31d7b2d591",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cgpotts/cs224u/blob/master/hw_openqa.ipynb)\n",
    "\n",
    "If Colab is opened with this badge, please **save a copy to drive** (from the File menu) before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8570fc5-2ac0-4c0e-b350-71990937ebd8",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da2d82-8c54-4d41-a59d-891f83f85f6e",
   "metadata": {},
   "source": [
    "The goal of this homework is to explore retrieval-augmented in-context learning. This is an exciting area that brings together a number of recent task ideas and modeling innovations. We will use the [DSPy programming library](http://dspy.ai) to build systems in this new mode.\n",
    "\n",
    "Our core task is __open-domain question answering (OpenQA)__. In this task, all that is given by the dataset is a question text, and the task is to answer that question. By contrast, in many modern QA tasks, the dataset provides a text and a gold passage, usually with a firm guarantee that the answer will be a substring of the passage.\n",
    "\n",
    "OpenQA is substantially harder than standard QA. The usual strategy is to use a _retriever_ to find passages in a large collection of texts and train a _reader_ to find answers in those passages. This means we have no guarantee that the retrieved passage will contain the answer we need. If we don't retrieve a passage containing the answer, our reader has no hope of succeeding. Although this is challenging, it is much more realistic and widely applicable than standard QA. After all, with the right retriever, an OpenQA system could be deployed over the entire Web.\n",
    "\n",
    "The task posed by this homework is harder even than OpenQA. We are calling this task __few-shot OpenQA__. The defining feature of this task is that the reader is simply a frozen, general purpose language model. It accepts string inputs (prompts) and produces text in response. It is not trained to answer questions per se, and nothing about its structure ensures that it will respond with a substring of the prompt corresponding to anything like an answer.\n",
    "\n",
    "__Few-shot QA__ (but not OpenQA!) is explored in the famous GPT-3 paper ([Brown et al. 2020](https://arxiv.org/abs/2005.14165)). The authors are able to get traction on the problem using GPT-3, an incredible finding. Our task here – __few-shot OpenQA__ – pushes this even further by retrieving passages to use in the prompt rather than assuming that the gold passage can be used in the prompt. If we can make this work, then it should be a major step towards flexibly and easily deploying QA technologies in new domains.\n",
    "\n",
    "In summary:\n",
    "\n",
    "| Task             | Passage given | Task-specific reader training |Task-specific retriever training  | \n",
    "|-----------------:|:-------------:|:-----------------------------:|:--------------------------------:|\n",
    "| QA               | yes           | yes                           | n/a                              |\n",
    "| OpenQA           | no            | yes                           | maybe                            |\n",
    "| Few-shot QA      | yes           | no                            | n/a                              |\n",
    "| Few-shot OpenQA  | no            | no                            | maybe                            | \n",
    "\n",
    "Just to repeat: your mission is to explore the final line in this table. The core notebook and assignment don't address the issue of training the retriever in a task-specific way, but this is something you could pursue for a final project; [the ColBERT codebase](https://github.com/stanford-futuredata/ColBERT) makes easy.\n",
    "\n",
    "It is a requirement of the bake-off that a general-purpose language model be used. In particular, trained QA systems cannot be used at all, and no fine-tuning is allowed either. See the original system question at the bottom of this message for guidance on which models are allowed.\n",
    "\n",
    "Note: the models we are working with here are _big_. This poses a challenge that is increasingly common in NLP: you have to pay one way or another. You can pay to use the GPT-3 API, or you can pay to use a local model on a heavy-duty cluster computer, or you can pay with time by using a local model on a more modest computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd32bb4-067f-4cd6-943f-3e5574400beb",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bcb0f-bc76-4277-a359-742d6dcee063",
   "metadata": {},
   "source": [
    "We have sought to make this notebook self-contained and easy to use on a personal computer, on Google Colab, and in Sagemaker Studio. For personal computer use, we assume you have already done everything in [setup.ipynb](setup.ipynb]). For cloud usage, the next few code blocks should handle all set-up steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b983bb-a20a-4c2a-9eee-9c553dd8c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import datasets\n",
    "    root_path = '.'\n",
    "except ModuleNotFoundError:\n",
    "    !git clone https://github.com/cgpotts/cs224u/\n",
    "    !pip install -r cs224u/requirements.txt\n",
    "    root_path = 'dspy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afb7791-92a3-474d-8c2f-807dc2441413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import dspy\n",
    "import warnings\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2651d-9868-45bf-81df-afafa832896a",
   "metadata": {},
   "source": [
    "### OpenAI set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04cb488-cd40-4f9d-b884-8ff83b012042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9da704b-d27b-480a-93b5-e16cf7c51803",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(root_path, 'cache')\n",
    "\n",
    "\n",
    "# keep the API keys in a `.env` file in the local root directory\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv('OPENAI_API_KEY')  # use the .env file as it is a good practice to keep keys outside of one's code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4465db5-4572-457e-b053-10f0185536fe",
   "metadata": {},
   "source": [
    "### ColBERT set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63747138-ae9c-4fc6-b508-0ad411fd58ac",
   "metadata": {},
   "source": [
    "#### Pretrained ColBERT parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf06f38-3dd6-4171-8b72-f5703df140d8",
   "metadata": {},
   "source": [
    "For this set-up phase, we first download pretrained ColBERT parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4925831c-f4d2-4b6b-87ed-145125c8f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(\"data\", \"openqa\", \"colbertv2.0.tar.gz\")):\n",
    "    !mkdir -p data/openqa\n",
    "    # ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "    !curl -O https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P data/openqa/\n",
    "    !tar -xvzf data/openqa/colbertv2.0.tar.gz -C data/openqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89bd6c-5497-4808-b2cf-9e15d9c37104",
   "metadata": {},
   "source": [
    "If something went wrong with the above, you can just download the file https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz, unarchive it, and move the resulting `colbertv2.0` directory into the `data/openqa` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf501ee-290b-4007-b04d-c175c3124806",
   "metadata": {},
   "source": [
    "#### Prebuilt ColBERT index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e330d4c-3d9f-4182-9c40-37010a5e5afd",
   "metadata": {},
   "source": [
    "Then we download a prebuilt index that covers the domain of our task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4bc73b-6936-44e2-bc80-7aca800abd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_home = os.path.join(\"experiments\", \"notebook\", \"indexes\", \"cs224u.collection.2bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabe7137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiments/notebook/indexes/cs224u.collection.2bits'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d8dd51-d0d9-4d72-ae77-f00f6151889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(index_home):\n",
    "    !mkdir -p experiments/notebook/indexes\n",
    "    !curl -O https://web.stanford.edu/class/cs224u/data/cs224u.collection.2bits.tgz --output-dir experiments/notebook/indexes\n",
    "    !tar -xvzf experiments/notebook/indexes/cs224u.collection.2bits.tgz -C experiments/notebook/indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2443e3-efe6-444d-98bd-63fa2634d8ff",
   "metadata": {},
   "source": [
    "If something went wrong with the above, download the file https://web.stanford.edu/class/cs224u/data/cs224u.collection.2bits.tgz, unarchive it, and move the resulting `cs224u.collection.2bits` directory into the `experiments/notebook/indexes` directory (which you will probably need to create)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3e9ff-c3d4-4a02-acfa-0d9183bce133",
   "metadata": {},
   "source": [
    "#### ColBERT server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fde4ad-a0e2-49a2-b3ec-446c5cb95a95",
   "metadata": {},
   "source": [
    "The final step for ColBERT is to create a local retriever for use with DSPy. The preferred method for doing this is with a seperate server. \n",
    "\n",
    "__Local usage__: If you are working with this notebook locally and a having a CUDA based device, make sure you have the [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) installed to be able to serve it on GPU. Otherwise you would need to run it on CPU. The next steps are for GPU usage. \n",
    "Open a new terminal window, navigate to the same directory as this notebook, and enter the following code:\n",
    "\n",
    "```\n",
    "conda activate nlu\n",
    "git clone https://github.com/stanford-futuredata/ColBERT/ ;\n",
    "export INDEX_ROOT=experiments/notebook/indexes/cs224u.collection.2bits/ ;\n",
    "export INDEX_HOME=cs224u.collection.2bits ;\n",
    "export PORT=8888 ;\n",
    "python ColBERT/server.py \n",
    "```\n",
    "\n",
    "This will start the server and you should be all set.\n",
    "\n",
    "__Colab usage__: If you are working in a Colab _and you have a Pro account_, you can open a terminal from the lower left corner of the interface and then paste in the above code. If you don't have a Pro account, then it should work to uncomment the code in the following cell and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2542a5-610f-4e77-9f00-ec86b18e1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"INDEX_ROOT\"] = \"experiments/notebook/indexes/cs224u.collection.2bits/\"\n",
    "os.environ[\"INDEX_HOME\"] = \"cs224u.collection.2bits\"\n",
    "os.environ[\"PORT\"] = \"8888\"\n",
    "\n",
    "!git clone https://github.com/stanford-futuredata/ColBERT/\n",
    "\n",
    "!nohup python ColBERT/server.py &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56ea89-7055-401f-8afc-8e7687da284b",
   "metadata": {},
   "source": [
    "This will take a minute to get started, and you won't get feedback on its progress, unfortunately. You can check that a server is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07cae52-81d9-4ff4-9a3b-cf79cf2d74a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corvi42          47002   0.0  0.0 410059184    224 s022  R+    8:18PM   0:00.00 grep server.py\n",
      "corvi42          47000   0.0  0.0 410359344   2080 s022  Ss+   8:18PM   0:00.18 /bin/zsh -c ps aux | grep server.py\n",
      "corvi42          46905   0.0  4.5 414104752 1511968 s018  SN    8:17PM   0:15.16 python ColBERT/server.py\n"
     ]
    }
   ],
   "source": [
    "!ps aux | grep server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988243e-6985-4ccd-a40e-9a2475566ed3",
   "metadata": {},
   "source": [
    "You should see a mention of `ColBERT/server.py` in the output. \n",
    "\n",
    "Assuming the server is now running, we can connect to it:\n",
    "\n",
    "*Note*: if the ColBERT server is running on a separate server, you can update `127.0.0.1` with the server's IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "556f268f-1a71-476d-bb13-8093feadaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = dspy.ColBERTv2(url=\"http://127.0.0.1:8888/api/search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bbc99-fcb2-4b22-b09b-c35371611709",
   "metadata": {},
   "source": [
    "### DSPy set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fc500-bbab-48b7-a704-67c6d57bb09b",
   "metadata": {},
   "source": [
    "Here we establish the Language Model `lm` and Retriever Model `rm` that we will be using. The defaults for `lm` are just for development. You may want to develop using an inexpensive model and then do your final evalautions wih an expensive one. DSPy has support for a wide range of model APIs and local models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c118b014-e13f-433d-ad60-074636c7e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.OpenAI(model='gpt-3.5-turbo', api_key=openai_key)\n",
    "\n",
    "dspy.settings.configure(lm=lm, rm=rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35ada2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsp.modules.gpt3.GPT3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711785d7-6bb9-4041-92e6-cc5f9308477e",
   "metadata": {},
   "source": [
    "Here's a command you can run to see which OpenAI models are available; OpenAI has entered into an increasingly closed mode where many older models are not available, so there are likely to be some surprises lurking here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a859fbb-e985-4031-b8ed-34f3b034db8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '__setitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key \u001b[38;5;241m=\u001b[39m openai_key)\n\u001b[0;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(models)\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/resources/models.py:91\u001b[0m, in \u001b[0;36mModels.list\u001b[0;34m(self, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m     86\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncPage[Model]:\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Lists the currently available models, and provides basic information about each\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    one such as the owner and availability.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSyncPage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/_base_client.py:1332\u001b[0m, in \u001b[0;36mSyncAPIClient.get_api_list\u001b[0;34m(self, path, model, page, body, options, method)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_api_list\u001b[39m(\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1323\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1329\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1330\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncPageT:\n\u001b[1;32m   1331\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_api_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/_base_client.py:1183\u001b[0m, in \u001b[0;36mSyncAPIClient._request_api_list\u001b[0;34m(self, model, page, options)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n\u001b[1;32m   1181\u001b[0m options\u001b[38;5;241m.\u001b[39mpost_parser \u001b[38;5;241m=\u001b[39m _parser\n\u001b[0;32m-> 1183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/_base_client.py:1066\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/_base_client.py:1165\u001b[0m, in \u001b[0;36mSyncAPIClient._process_response\u001b[0;34m(self, cast_to, options, response, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(RAW_RESPONSE_HEADER)):\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, api_response)\n\u001b[0;32m-> 1165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/_response.py:325\u001b[0m, in \u001b[0;36mAPIResponse.parse\u001b[0;34m(self, to)\u001b[0m\n\u001b[1;32m    323\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(to\u001b[38;5;241m=\u001b[39mto)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mpost_parser):\n\u001b[0;32m--> 325\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, BaseModel):\n\u001b[1;32m    328\u001b[0m     add_request_id(parsed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_id)\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/_base_client.py:1174\u001b[0m, in \u001b[0;36mSyncAPIClient._request_api_list.<locals>._parser\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parser\u001b[39m(resp: SyncPageT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncPageT:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_private_attributes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/openai/_base_client.py:212\u001b[0m, in \u001b[0;36mBaseSyncPage._set_private_attributes\u001b[0;34m(self, client, model, options)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_private_attributes\u001b[39m(\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    208\u001b[0m     client: SyncAPIClient,\n\u001b[1;32m    209\u001b[0m     model: Type[_T],\n\u001b[1;32m    210\u001b[0m     options: FinalRequestOptions,\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m client\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options \u001b[38;5;241m=\u001b[39m options\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/pydantic/main.py:991\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# if None is returned from _setattr_handler, the attribute was set directly\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (setattr_handler \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setattr_handler(name, value)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 991\u001b[0m     \u001b[43msetattr_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# call here to not memo on possibly unknown fields\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_setattr_handlers__[name] \u001b[38;5;241m=\u001b[39m setattr_handler\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/nlu/lib/python3.9/site-packages/pydantic/main.py:105\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(model, name, val)\u001b[0m\n\u001b[1;32m     98\u001b[0m     model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[name] \u001b[38;5;241m=\u001b[39m val\n\u001b[1;32m     99\u001b[0m     model\u001b[38;5;241m.\u001b[39m__pydantic_fields_set__\u001b[38;5;241m.\u001b[39madd(name)\n\u001b[1;32m    102\u001b[0m _SIMPLE_SETATTR_HANDLERS: Mapping[\u001b[38;5;28mstr\u001b[39m, Callable[[BaseModel, \u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_field\u001b[39m\u001b[38;5;124m'\u001b[39m: _model_field_setattr_handler,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidate_assignment\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m model, name, val: model\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_assignment(model, name, val),  \u001b[38;5;66;03m# pyright: ignore[reportAssignmentType]\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprivate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m model, name, val: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_private__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m(name, val),  \u001b[38;5;66;03m# pyright: ignore[reportOptionalMemberAccess]\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcached_property\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m model, name, val: model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setitem__\u001b[39m(name, val),\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra_known\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m model, name, val: _object_setattr(model, name, val),\n\u001b[1;32m    108\u001b[0m }\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseModel\u001b[39;00m(metaclass\u001b[38;5;241m=\u001b[39m_model_construction\u001b[38;5;241m.\u001b[39mModelMetaclass):\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"!!! abstract \"Usage Documentation\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        [Models](../concepts/models.md)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m        __pydantic_private__: Values of private attributes set on the model instance.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '__setitem__'"
     ]
    }
   ],
   "source": [
    "# client = OpenAI(api_key = openai_key)\n",
    "# models = client.models.list()\n",
    "# print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b3dc2-87d7-4b8b-b603-ee567e008710",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de295d35-fea5-46d2-9a01-022ad88e54cd",
   "metadata": {},
   "source": [
    "Our core development dataset is [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/). We chose this dataset because it is well-known and widely used, and it is large enough to support lots of meaningful development work, without, though, being so large as to require lots of compute power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eaf2fd0-d060-4100-8702-f7311efd6129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd9b4f7ed5b462ca642122956c2ddee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be412dd102b40819385d28a869f9f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feed73879b73480ea22d61b972e24955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38826668cd5647eb99f6fb858d0c8c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdac953e2edf46a992dda89aecf297fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36965402-e3da-4531-b7e9-4b12cebcdf30",
   "metadata": {},
   "source": [
    "The following utility just reads a SQuAD split in as a list of `dspy.Example` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21ad3e0b-7662-43b8-9409-a1a57442458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squad_split(squad, split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Use `split='train'` for the train split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dspy.Example with attributes question, answer\n",
    "\n",
    "    \"\"\"\n",
    "    data = zip(*[squad[split][field] for field in squad[split].features])\n",
    "    exs = [dspy.Example(question=q, answer=a['text'][0]).with_inputs(\"question\")\n",
    "           for eid, title, context, q, a in data]\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3847d38-4e70-46b7-bf46-4c8b784c5ee5",
   "metadata": {},
   "source": [
    "### SQuAD train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c91e1-586b-4747-be39-3092e60f182f",
   "metadata": {},
   "source": [
    "To build few-shot prompts, we will often sample SQuAD train examples, so we load that split here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66c4feba-d580-4984-a449-0b92a53ef13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = get_squad_split(squad, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab8c41-8eae-4d15-ad4d-e28b3c58eb4a",
   "metadata": {},
   "source": [
    "### SQuAD dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37198b33-c47b-4e0e-af8b-c00860658cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dev = get_squad_split(squad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40c768-57cc-4a07-a3ef-5e34262b0ace",
   "metadata": {},
   "source": [
    "### SQuAD dev sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636601b5-c7ad-4177-a6d6-f3afdb0bedae",
   "metadata": {},
   "source": [
    "Evaluations are expensive in this new era! Here's a small sample to use for dev assessments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64ccdd0e-de78-440d-bfbe-358c12eada5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "dev_exs = random.sample(squad_dev, k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f524f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': \"In 1517 who was Luther's bishop?\", 'answer': 'Albert of Mainz'}) (input_keys={'question'}),\n",
       " Example({'question': \"When was the construction that changed the Rhine's Delta?\", 'answer': '20th Century'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many companies were registered in Warsaw in 2006?', 'answer': '304,016'}) (input_keys={'question'}),\n",
       " Example({'question': \"What is the CJEU's duty?\", 'answer': 'to \"ensure that in the interpretation and application of the Treaties the law is observed\"'}) (input_keys={'question'}),\n",
       " Example({'question': 'What would a teacher do for someone who is cocky?', 'answer': 'deflate'}) (input_keys={'question'}),\n",
       " Example({'question': \"What cancer researchers were also apart of the university's faculty?\", 'answer': 'Charles Brenton Huggins and Janet Rowley'}) (input_keys={'question'}),\n",
       " Example({'question': 'What do capitalist firms substitute equipment for in a Marxian analysis?', 'answer': 'labor inputs'}) (input_keys={'question'}),\n",
       " Example({'question': 'In what year did the film also mention the number of regenerations?', 'answer': '1996'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who continued to hold the citadel of Bukhara after the Mongols took the rest of the city?', 'answer': 'a unit of Turkish defenders'}) (input_keys={'question'}),\n",
       " Example({'question': \"What scientific field's theory has received contributions from the steam engine?\", 'answer': 'thermodynamic'}) (input_keys={'question'}),\n",
       " Example({'question': 'What kind of engines did the biplane design have?', 'answer': 'turbine engines'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many quadrangles does the Main Quadrangles have?', 'answer': 'six'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which building was the NFL Experience held at for Super Bowl 50?', 'answer': 'Moscone Center'}) (input_keys={'question'}),\n",
       " Example({'question': 'What are new responsibilities pharmacy technicians now deal with?', 'answer': \"patients' prescriptions and patient safety issues\"}) (input_keys={'question'}),\n",
       " Example({'question': 'In South Africa, along with privately governed schools, what schools are classified as independent?', 'answer': 'traditional private'}) (input_keys={'question'}),\n",
       " Example({'question': 'When did Germany found their first settlement?', 'answer': '1884'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who did the Broncos prevent from going to the Super Bowl?', 'answer': 'New England Patriots'}) (input_keys={'question'}),\n",
       " Example({'question': 'What was Jacksonville referred to as after the consolidation?', 'answer': '\"Bold New City of the South\"'}) (input_keys={'question'}),\n",
       " Example({'question': 'Scientists disagree with how the Amazon rainforest changed over time with some arguing that it was reduced to isolated refugia seperated by what?', 'answer': 'open forest and grassland'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is one of the reason that US production has been held responsible for recessions and lower economic growth?', 'answer': 'distribution and price disruptions'}) (input_keys={'question'}),\n",
       " Example({'question': 'Where was the Muslim Brotherhood founded?', 'answer': 'Ismailiyah, Egypt'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many types of science fiction have been impacted by Tesla?', 'answer': 'several'}) (input_keys={'question'}),\n",
       " Example({'question': \"How man of Grainger Town's 450 buildings are listed?\", 'answer': '244'}) (input_keys={'question'}),\n",
       " Example({'question': 'Where was the new media day event for Super Bowl 50 held?', 'answer': 'SAP Center in San Jose.'}) (input_keys={'question'}),\n",
       " Example({'question': 'How old was Newton during Super Bowl 50?', 'answer': '26'}) (input_keys={'question'}),\n",
       " Example({'question': \"At what university's facility did the Panthers practice?\", 'answer': 'San Jose State'}) (input_keys={'question'}),\n",
       " Example({'question': 'What distinguishes stromal thylakoids?', 'answer': 'are in contact with the stroma'}) (input_keys={'question'}),\n",
       " Example({'question': 'What date were the top two stadium choices for Super Bowl 50 announced?', 'answer': 'October 16, 2012'}) (input_keys={'question'}),\n",
       " Example({'question': \"Before which military campaign did Chagatai publicly dispute Jochi's paternity?\", 'answer': 'invasion of the Khwarezmid Empire'}) (input_keys={'question'}),\n",
       " Example({'question': 'Of what were materials that left little residue thought to contain?', 'answer': 'phlogiston'}) (input_keys={'question'}),\n",
       " Example({'question': 'What three things are needed for construction to take place?', 'answer': 'planning,[citation needed] design, and financing'}) (input_keys={'question'}),\n",
       " Example({'question': 'How much money is being spent on other Super Bowl-related events?', 'answer': '$2 million'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the nickname for the \"Millennial Northern Hemisphere temperature reconstruction\" graph?', 'answer': 'the \"hockey stick graph\"'}) (input_keys={'question'}),\n",
       " Example({'question': 'At what temperature will oxygen condense?', 'answer': '90.20 K'}) (input_keys={'question'}),\n",
       " Example({'question': 'When was the color crimson adopted at Harvard as official color?', 'answer': '1875'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the Chinese name for the Yuan dynasty?', 'answer': 'Yuán Cháo'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the density of all primes compatible with a modulo 9?', 'answer': '1/6'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which year did the price of oil drop to $10 per barrel?', 'answer': '1980s'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which British sculptor whose work include the Queen Victoria memorial in front of Buckingham Palace is included in the V&A collection?', 'answer': 'Thomas Brock'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who backed policies that have solutions that sound good but have poor prospects?', 'answer': 'congresses and presidents'}) (input_keys={'question'}),\n",
       " Example({'question': 'In what year did Dewar experiment on liquid oxygen?', 'answer': '1891'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is almost identical across all nations and jurisdictions?', 'answer': 'homicides'}) (input_keys={'question'}),\n",
       " Example({'question': 'What was the name given to a section of Kearney Boulevard in efforts to change the areas image?', 'answer': 'Brookhaven'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who is the General Manager for the Broncos?', 'answer': 'John Elway'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the goal of individual civil disobedience?', 'answer': 'render certain laws ineffective'}) (input_keys={'question'}),\n",
       " Example({'question': 'In what theory is the idea of a number exchanged with that of an ideal?', 'answer': 'In ring theory'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many times less is the strenght of the weak field compared to the strong?', 'answer': '1013'}) (input_keys={'question'}),\n",
       " Example({'question': 'What subatomic particle did Tesla deny the existence of?', 'answer': 'electron'}) (input_keys={'question'}),\n",
       " Example({'question': 'When did this attempt take place?', 'answer': '1560'}) (input_keys={'question'}),\n",
       " Example({'question': 'When did England formally declare war on France?', 'answer': 'May 18, 1756'}) (input_keys={'question'}),\n",
       " Example({'question': 'WHat does UserDatagram Protocol gaurentee', 'answer': 'In the virtual call system, the network guarantees sequenced delivery of data to the host'}) (input_keys={'question'}),\n",
       " Example({'question': 'What country has higher scores on standardized tests than the U.S.?', 'answer': 'Japan'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who designed the ceiling and stained-glass windows of the Green Dining Room?', 'answer': 'Edward Burne-Jones'}) (input_keys={'question'}),\n",
       " Example({'question': \"What did 'Da Yuan Tong Zhi' mean?\", 'answer': '\"the comprehensive institutions of the Great Yuan\"'}) (input_keys={'question'}),\n",
       " Example({'question': \"What Mongolian system did Kublai's government compromise with?\", 'answer': 'patrimonial feudalism'}) (input_keys={'question'}),\n",
       " Example({'question': 'Where did Huguenots and Walloons settle in England?', 'answer': 'Canterbury'}) (input_keys={'question'}),\n",
       " Example({'question': 'When did the y. pestis reach England?', 'answer': 'spring of 1349'}) (input_keys={'question'}),\n",
       " Example({'question': 'Why are ctenophores extremely rare as fossils?', 'answer': 'Because of their soft, gelatinous bodies'}) (input_keys={'question'}),\n",
       " Example({'question': 'What did the Salafi movement put emphasis on?', 'answer': 'sharia rather than the building of Islamic institutions,'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many people in Quzhou are descended from Confucius?', 'answer': '30,000'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which khanates had converted to Islam?', 'answer': 'western'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many drugs approved by the FDA in 2013 were specialty drugs?', 'answer': '19'}) (input_keys={'question'}),\n",
       " Example({'question': 'What type of interpretation of Islam does Salafism promote?', 'answer': 'conservative'}) (input_keys={'question'}),\n",
       " Example({'question': 'On what game console was the CBS Sports app available?', 'answer': 'Xbox One'}) (input_keys={'question'}),\n",
       " Example({'question': \"Who played Doctor Who on stage in the 70's?\", 'answer': 'Trevor Martin'}) (input_keys={'question'}),\n",
       " Example({'question': 'What happened during the plugs-out test during the delay for the spacesuit odor? ', 'answer': 'electrical fire'}) (input_keys={'question'}),\n",
       " Example({'question': 'Plants lack what kind of immune cells?', 'answer': 'phagocytic cells'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who claims that public companies can also be part of civil disobedience?', 'answer': 'Brownlee'}) (input_keys={'question'}),\n",
       " Example({'question': 'What year did BSkyB and Microsoft announce their settlement?', 'answer': '2013'}) (input_keys={'question'}),\n",
       " Example({'question': \"Which of ABC's main production facilities is located in Hollywood, CA?\", 'answer': 'ABC Television Center'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many prime numbers exist?', 'answer': 'infinitely many'}) (input_keys={'question'}),\n",
       " Example({'question': \"What year did Börte's give birth to Jochi?\", 'answer': '1185'}) (input_keys={'question'}),\n",
       " Example({'question': 'Along with the American Institute of Electrical Engineers what other institute eventually became the IEEE?', 'answer': 'the Institute of Radio Engineers'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the duration of Harvard Academic year?', 'answer': 'beginning in early September and ending in mid-May'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who founded the Ming dynasty?', 'answer': 'Zhu Yuanzhang'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which bound of time is more difficult to establish?', 'answer': 'lower bounds'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which newspaper defined southern California?', 'answer': 'Los Angeles Times'}) (input_keys={'question'}),\n",
       " Example({'question': 'When did Kenya gain independance?', 'answer': '12 December 1963'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the fastest growing area in the pharmaceutical industry?', 'answer': 'specialty pharmacies'}) (input_keys={'question'}),\n",
       " Example({'question': 'Most of the productions in the TGIF lineup were produced by what production company?', 'answer': 'Miller-Boyett Productions'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the name of the private day school for K-12 students the university runs?', 'answer': 'University of Chicago Laboratory Schools'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who may have been called upon to fund the festival associated with the Super Bowl in Santa Clara?', 'answer': 'city council'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many of the survey respondents considered Doctor Who very unsuitable for family viewing?', 'answer': '3%'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who sacked Newton a few plays after the challenge?', 'answer': 'Von Miller'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which principle is based on the appearance of fossils in sedimentary rocks?', 'answer': 'The principle of faunal succession'}) (input_keys={'question'}),\n",
       " Example({'question': 'Historically, which movement has the Methodist Church supported?', 'answer': 'temperance movement'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many Muslims came from around the world to fight in Afghanistan?', 'answer': '16,000 to 35,000'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who represents the Scottish Parliament at home and abroad in an official capacity?', 'answer': 'The Presiding Officer'}) (input_keys={'question'}),\n",
       " Example({'question': 'How are pharmacists regulated in most jurisdictions?', 'answer': 'separately from physicians'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which team was suspended from the MLS?', 'answer': 'Chivas USA'}) (input_keys={'question'}),\n",
       " Example({'question': 'Jamboree Business Parks belongs to which business center?', 'answer': 'West Irvine'}) (input_keys={'question'}),\n",
       " Example({'question': 'What uprising began in 1351?', 'answer': 'the Red Turban Rebellion'}) (input_keys={'question'}),\n",
       " Example({'question': 'What was another term used for the oil crisis?', 'answer': 'first oil shock'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many Panthers went to the Pro Bowl?', 'answer': 'Ten'}) (input_keys={'question'}),\n",
       " Example({'question': 'What device is used to recycle the boiler water in most steam engines?', 'answer': 'water pump'}) (input_keys={'question'}),\n",
       " Example({'question': 'How large are Cytoplasmic ribosomes?', 'answer': '25 nm'}) (input_keys={'question'}),\n",
       " Example({'question': 'What theorem defines the main role of primes in number theory?', 'answer': 'The fundamental theorem of arithmetic'}) (input_keys={'question'}),\n",
       " Example({'question': 'Name an extra that was added to the production of the compacts.', 'answer': 'power steering'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is a kind of defense response that makes the entire plant resistant to a particular agent?', 'answer': 'Systemic acquired resistance (SAR)'}) (input_keys={'question'}),\n",
       " Example({'question': 'What does the CPI scale measure?', 'answer': 'gauge the prevalence of public sector corruption in various countries'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who painted the retable of St. George that is in the V&A collection?', 'answer': 'Andrés Marzal De Sax'}) (input_keys={'question'}),\n",
       " Example({'question': 'What type of ballot is used to elect the Presiding Officer and deputies of the Parliament?', 'answer': 'secret'}) (input_keys={'question'}),\n",
       " Example({'question': 'When UPT bough ABC, what was the merged company called?', 'answer': 'American Broadcasting-Paramount Theatres, Inc'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who lives no longer on average than Greeks and New Zealanders?', 'answer': 'Americans'}) (input_keys={'question'}),\n",
       " Example({'question': 'In lands attributed to what tribe are found remains of large settlements?', 'answer': 'Xingu tribe'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is reduced by using plastid transformation for gene modification?', 'answer': 'environmental risks'}) (input_keys={'question'}),\n",
       " Example({'question': 'Rudyard Kipling was an influential spokesman for what?', 'answer': 'The British spirit of imperialism'}) (input_keys={'question'}),\n",
       " Example({'question': 'What ranking does the Super Bowl 50 halftime show have on the list of most watched TV broadcasts?', 'answer': 'third'}) (input_keys={'question'}),\n",
       " Example({'question': 'What do some modern historians claim Genghis Khan sought to add his legal code at the end of his reign?', 'answer': 'legal equality of all individuals, including women'}) (input_keys={'question'}),\n",
       " Example({'question': 'How did the settlers protect their interests?', 'answer': 'banned the growing of coffee, introduced a hut tax, and the landless were granted less and less land in exchange for their labour'}) (input_keys={'question'}),\n",
       " Example({'question': 'In general, what were teachers paid in the past?', 'answer': 'relatively low salaries'}) (input_keys={'question'}),\n",
       " Example({'question': 'How much does petroleum account for of the national import bill?', 'answer': 'r 20% to 25%'}) (input_keys={'question'}),\n",
       " Example({'question': 'Besides Germany and Switzerland, where else is Lake Constance?', 'answer': 'Austria'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the weight of a bushel of coal in pounds?', 'answer': '94'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is a PPP also known as?', 'answer': 'private finance initiatives (PFIs)'}) (input_keys={'question'}),\n",
       " Example({'question': 'What city, raided by clans and dukes, preceded the founding of Warszowa?', 'answer': 'Jazdów'}) (input_keys={'question'}),\n",
       " Example({'question': 'Why did the series end in 2011?', 'answer': 'due to the death of Elisabeth Sladen'}) (input_keys={'question'}),\n",
       " Example({'question': 'When were the Financial Interest and Syndication Rules repealed?', 'answer': '1993'}) (input_keys={'question'}),\n",
       " Example({'question': 'The Rhine and what other river drained the northern flanks of the alps?', 'answer': 'Danube'}) (input_keys={'question'}),\n",
       " Example({'question': 'When was it discovered that prime numbers could applied to the creation of public key cryptography algorithms?', 'answer': 'the 1970s'}) (input_keys={'question'}),\n",
       " Example({'question': 'What are engines using four expansion stages known as?', 'answer': 'quadruple expansion engines'}) (input_keys={'question'}),\n",
       " Example({'question': \"What was Tugh's Chinese-style name?\", 'answer': 'Emperor Wenzong'}) (input_keys={'question'}),\n",
       " Example({'question': 'What group of people cannot be part of civil disobedience?', 'answer': 'sovereign branches of government'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who is parodied on programs such as Saturday Night Live and The Simpsons?', 'answer': 'Doctor Who fandom'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who did Disney sell the four newspapers that ABC controlled to?', 'answer': 'Knight Ridder'}) (input_keys={'question'}),\n",
       " Example({'question': 'What reason is given that you should also protest public companies?', 'answer': 'a larger challenge to the legal system'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which museum was among those that loaned more modern works for the new sculpture galleries?', 'answer': 'Tate Britain'}) (input_keys={'question'}),\n",
       " Example({'question': 'Super Bowl 50 determined the NFL champion for what season?', 'answer': '2015'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is evidence chloroplasts descended from endosymbiotic cyanobacteria?', 'answer': 'a double membrane'}) (input_keys={'question'}),\n",
       " Example({'question': 'What are Plastoglobuli attached to?', 'answer': 'either to a thylakoid or to another plastoglobulus attached to a thylakoid'}) (input_keys={'question'}),\n",
       " Example({'question': 'What time framd does the Seven Years War cover?', 'answer': 'declaration of war in 1756 to the signing of the peace treaty in 1763'}) (input_keys={'question'}),\n",
       " Example({'question': 'When was the United Methodist Church created?', 'answer': 'April 23, 1968'}) (input_keys={'question'}),\n",
       " Example({'question': 'When was the Britain Can Make It exhibition held?', 'answer': 'between September and November 1946'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is lower in countries with more inequality for the top 21 industrialized countries?', 'answer': 'life expectancy'}) (input_keys={'question'}),\n",
       " Example({'question': 'Western Imperialism divided the globe according to which theory?', 'answer': 'the world systems theory'}) (input_keys={'question'}),\n",
       " Example({'question': 'In what city is the Moscone Center located?', 'answer': 'San Francisco'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who wanted Israel to withdraw from its border?', 'answer': 'Ted Heath'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the magnitude of force divided by when external force is added?', 'answer': 'mass of the system'}) (input_keys={'question'}),\n",
       " Example({'question': 'What could the Supplemental Nutrition Assistance Program purchase?', 'answer': 'essentials'}) (input_keys={'question'}),\n",
       " Example({'question': 'Of what form do Fermat numbers take?', 'answer': '22n + 1'}) (input_keys={'question'}),\n",
       " Example({'question': 'What enables the Scottish Parliament to scrutinize the government?', 'answer': 'Several procedures'}) (input_keys={'question'}),\n",
       " Example({'question': 'When was the heatwave in which Hopetoun recorded its highest temperature?', 'answer': '2009'}) (input_keys={'question'}),\n",
       " Example({'question': 'How much correspondence did Tesla send Morgan in the five years following 1901?', 'answer': 'over 50 letters'}) (input_keys={'question'}),\n",
       " Example({'question': 'Trial division involves dividing n by every integer m greater than what?', 'answer': 'greater than 1'}) (input_keys={'question'}),\n",
       " Example({'question': 'What were the years two Regulations that conflicted with an Italian law originate in the Simmenthal SpA case? ', 'answer': '1964 and 1968'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who is the designer of the \"50?\"', 'answer': 'Tiffany & Co'}) (input_keys={'question'}),\n",
       " Example({'question': 'What architecture type came before Norman in England?', 'answer': 'Anglo-Saxon'}) (input_keys={'question'}),\n",
       " Example({'question': 'Where did Tesla work in 1888?', 'answer': 'Pittsburgh'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who had the most rushing yards on the Broncos?', 'answer': 'C. J. Anderson'}) (input_keys={'question'}),\n",
       " Example({'question': \"When did economists reach a conclusion with the S&P's rating agency?\", 'answer': '2014'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many tackles did Luke Kuechly register?', 'answer': '118'}) (input_keys={'question'}),\n",
       " Example({'question': 'What do lobates feed on?', 'answer': 'suspended planktonic prey'}) (input_keys={'question'}),\n",
       " Example({'question': 'What are some supplementary sources of European Union law?', 'answer': 'case law by the Court of Justice, international law and general principles of European Union law'}) (input_keys={'question'}),\n",
       " Example({'question': 'Geoglyphs dating to what period were found in deforested land along the Amazon River?', 'answer': 'AD 0–1250'}) (input_keys={'question'}),\n",
       " Example({'question': 'What theorems are responsible for determining questions of time and space requirements?', 'answer': 'time and space hierarchy theorems'}) (input_keys={'question'}),\n",
       " Example({'question': \"Who was on Celeron's expedition?\", 'answer': '200 Troupes de la marine and 30 Indians'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who is the current Premier of Victoria?', 'answer': 'Daniel Andrews'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who created the photographic series titled Animal Locomotion?', 'answer': 'Eadweard Muybridge'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many seats does the amphitheatre at Woodward park have?', 'answer': '2,500'}) (input_keys={'question'}),\n",
       " Example({'question': \"What were Isaac's chains made out of?\", 'answer': 'silver'}) (input_keys={'question'}),\n",
       " Example({'question': \"Southern California's economy can be described as one of the largest in the United States and what other characteristic?\", 'answer': 'diverse'}) (input_keys={'question'}),\n",
       " Example({'question': 'In contrast how were Catholic saints portrayed?', 'answer': 'frail Catholic saints'}) (input_keys={'question'}),\n",
       " Example({'question': 'What was the reason the Italian Constitutional court gave that resulted in Mr. Costa losing his his claim against ENEL?', 'answer': 'nationalisation law was from 1962, and the treaty was in force from 1958'}) (input_keys={'question'}),\n",
       " Example({'question': 'What was the topic of the error?', 'answer': 'Himalayan glaciers'}) (input_keys={'question'}),\n",
       " Example({'question': 'Downtown Santa Monica and Downtown Glendale are a part of which area?', 'answer': 'Los Angeles Area'}) (input_keys={'question'}),\n",
       " Example({'question': 'Tribal members living in the rainforests of what region are using Google Earth?', 'answer': 'southern Suriname'}) (input_keys={'question'}),\n",
       " Example({'question': 'How is packet mode communication implemented ', 'answer': 'with or without intermediate forwarding nodes'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the profession of Jake Rosenfield?', 'answer': 'Sociologist'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who lectured at the Newcastle library on the 20th of October in 1880?', 'answer': 'Joseph Swan'}) (input_keys={'question'}),\n",
       " Example({'question': 'What was the Yuan dynasty called in Mongolian?', 'answer': 'Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus'}) (input_keys={'question'}),\n",
       " Example({'question': 'In what episode did Doctor Who acknowledge having had a brother?', 'answer': 'Smith and Jones'}) (input_keys={'question'}),\n",
       " Example({'question': 'In 1962, who was responsible for the authorship of a paper published on real time-computations?', 'answer': 'Hisao Yamada'}) (input_keys={'question'}),\n",
       " Example({'question': \"Who slipped on the Levi's Stadium turf in week 6 of the 2015 NFL season?\", 'answer': 'Justin Tucker'}) (input_keys={'question'}),\n",
       " Example({'question': 'What can be combined with geophysical data to produce a better view of the subsurface?', 'answer': 'well logs'}) (input_keys={'question'}),\n",
       " Example({'question': \"In which year did Genghis Khan's grandson invade Kievan Rus'?\", 'answer': '1237'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which Member of Parliament explained how the museum would preserve the collection and keep it available to the public?', 'answer': 'Bryan Davies'}) (input_keys={'question'}),\n",
       " Example({'question': 'What US war has a large amount of Civil Disobedients?', 'answer': 'Vietnam War'}) (input_keys={'question'}),\n",
       " Example({'question': 'What group specifically opposed the Huguenots?', 'answer': 'Catholic Church in France'}) (input_keys={'question'}),\n",
       " Example({'question': 'What year was the Treaty of Amsterdam created?', 'answer': '1997'}) (input_keys={'question'}),\n",
       " Example({'question': 'What thesis specifies that a polynomial relationship exists within time complexities in a computational model? ', 'answer': 'Cobham-Edmonds thesis'}) (input_keys={'question'}),\n",
       " Example({'question': 'The legislative body, the Council, are made up of what type of individuals?', 'answer': 'different ministers of the member states'}) (input_keys={'question'}),\n",
       " Example({'question': 'How is the climate near the savannah grasslands?', 'answer': 'The climate is cooler'}) (input_keys={'question'}),\n",
       " Example({'question': \"What did Watt add to Newcomen's engine between 1763 and 1775?\", 'answer': 'condenser'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who chartered the British East India Company?', 'answer': 'Queen Elizabeth'}) (input_keys={'question'}),\n",
       " Example({'question': \"What religion's schools were integrated into New Zealand public schools between 1979 and 1984?\", 'answer': 'Catholic'}) (input_keys={'question'}),\n",
       " Example({'question': 'What game did Thomas Davis say he would play in, despite breaking a bone earlier on?', 'answer': 'Super Bowl'}) (input_keys={'question'}),\n",
       " Example({'question': 'What function do compounds like phenol and acetone serve in the manufacture of many other substances?', 'answer': 'feeder materials'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many picks did Aqib Talib have?', 'answer': 'three'}) (input_keys={'question'}),\n",
       " Example({'question': 'What even is the earliest known reference to immunity?', 'answer': 'plague of Athens in 430 BC'}) (input_keys={'question'}),\n",
       " Example({'question': \"What translation of Luther's is still used today?\", 'answer': 'the Bible'}) (input_keys={'question'}),\n",
       " Example({'question': 'Which late night comedy host show played immediately after Super Bowl 50 ended?', 'answer': 'The Late Show with Stephen Colbert'}) (input_keys={'question'}),\n",
       " Example({'question': 'Despite being traditionall described as \"eight counties\", how many counties does this region actually have?', 'answer': '10 counties'}) (input_keys={'question'}),\n",
       " Example({'question': 'What river separates Jacksonville?', 'answer': 'The St. Johns River'}) (input_keys={'question'}),\n",
       " Example({'question': 'What type of medicine did Mongol shamans use?', 'answer': 'spiritual cures'}) (input_keys={'question'}),\n",
       " Example({'question': 'Who selects and hires the best ideas and appropriate contractors?', 'answer': 'The owner'}) (input_keys={'question'}),\n",
       " Example({'question': 'How concentrated do the hydrogen ions get in the thylakoid space?', 'answer': 'up to a thousand times'}) (input_keys={'question'}),\n",
       " Example({'question': 'How much heavier is oxygen 18 than oxygen 16?', 'answer': '12%'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many Frenchman won Battle of Carillon?', 'answer': '3,600'}) (input_keys={'question'}),\n",
       " Example({'question': \"What was East and Central Africa's economy boosted by?\", 'answer': 'rapid expansion in telecommunication and financial activity'}) (input_keys={'question'}),\n",
       " Example({'question': 'Why does competition among workers drive down wages?', 'answer': 'expendable nature of the worker'}) (input_keys={'question'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_exs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28265d01-890d-4f04-b518-da0e8a1cb235",
   "metadata": {},
   "source": [
    "## DSPy basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b9d12-fc8c-4aae-b09e-0d72a4aa54f5",
   "metadata": {},
   "source": [
    "### LM usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278daac-11f9-4327-a06f-1c408a06a71d",
   "metadata": {},
   "source": [
    "Here's the most basic way to use the LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02364ed6-3c6d-4eaf-849a-2d9e30d84b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The birthplace of the first author to win a Hugo Award for a translation is Japan. The author is Yoshio Kobayashi, who won the Hugo Award for Best Novel in 1980 for his translation of \"The Left Hand of Darkness\" by Ursula K. Le Guin.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"What is the birthplace of the first author to win a Hugo Award for a translation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356d9a8-750b-4383-bc5b-4173ca5c13ac",
   "metadata": {},
   "source": [
    "Keyword arguments to the underlying LM are passed through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19ab8deb-3b9d-4f57-bd70-7c170be294c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hawaii and Alaska are the only two states that do not share a border with any other U.S. states.',\n",
       " 'Alaska and Hawaii are the only two U.S. states that do not border any other U.S. states.',\n",
       " 'Hawaii and Alaska do not share a border with any other U.S. states.',\n",
       " 'Hawaii and Alaska are the only two U.S. states that do not share a border with another U.S. state.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"Which U.S. states border no U.S. states?\", temperature=0.9, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e8d99-6d49-420d-ab5d-cc01b53cd4a1",
   "metadata": {},
   "source": [
    "With `lm.inspect_history`, we can see the most recent language model calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f7cb5a5-3a3f-4e78-b9af-488fadc896ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Which U.S. states border no U.S. states?\u001b[32m Hawaii and Alaska are the only two states that do not share a border with any other U.S. states.\u001b[0m\u001b[31m \t (and 3 other completions)\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0f2df-df1f-422d-bff3-6c4a3d947f6e",
   "metadata": {},
   "source": [
    "### Signature-based prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0888a2a-fcaa-44b0-beef-b097c856c74b",
   "metadata": {},
   "source": [
    "In DSPy, __signatures__ are declarative statements about what we want the model to do. In the following `\"question -> answer\"` is the signature (the most basic QA signature one could write), and `dspy.Predict` is used to turn this into a complete QA system: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11f04cba-7d46-4680-a154-a0062243618e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basic_predictor = dspy.Predict(\"question -> answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994fc10-8c28-4836-bd7c-008a9a3d34e4",
   "metadata": {},
   "source": [
    "Here we use `basic_predictor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfe102d0-c0e5-45ad-aae8-51f6ea6e70f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Paris, France'\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_predictor(question=\"What is the birthplace of the first author to win a Hugo Award for a translation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad1a1b-d422-46df-8b03-4054bfec5fc1",
   "metadata": {},
   "source": [
    "And here is the prompt that was given to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af4f86db-97d5-4742-8468-4627de12f181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the birthplace of the first author to win a Hugo Award for a translation?\n",
      "Answer:\u001b[32m Paris, France\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845f62b-8af3-4fa2-af8d-f4ddd1181f30",
   "metadata": {},
   "source": [
    "In many cases, we will want more control over the prompt. Writing a small custom `dspy.Signature` class is the easiest way to accomplish this. In the following, we just just tweak the initial instruction and provide some formatting guidance for the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9dda770-e7ca-4682-b283-d4f4525adb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicQASignature(dspy.Signature):\n",
    "    __doc__ = \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83f7297a-b174-4566-8748-6b66d515ed25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sig_predictor = dspy.Predict(BasicQASignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30a02d55-fac7-43c3-851b-3e21f06df14d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Maine, Hawaii'\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_predictor(question=\"Which U.S. states border no U.S. states?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f613c05-ec78-4129-ac44-71bac4e253ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "Answer: often between 1 and 5 words\n",
      "\n",
      "---\n",
      "\n",
      "Question: Which U.S. states border no U.S. states?\n",
      "Answer:\u001b[32m Maine, Hawaii\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56169f83-0df6-46dc-b617-fadff1b96132",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244aed0-403e-4707-a9eb-e29fc1e4dc57",
   "metadata": {},
   "source": [
    "One of the hallmarks of DSPy is that it adopts design patterns from PyTorch. The main example of this is DSPy's use of the `Module` as the basic unit for writing simple and complex programs. Here is a very basic module for QA that makes use of `BasicQASignature` as we defined it just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "506c9946-fee4-4dc3-a05e-767f85c25292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(BasicQASignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.generate_answer(question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921aaed-6351-443d-8478-6e9a64b49d45",
   "metadata": {},
   "source": [
    "As with PyTorch, the `forward` method is called when we want to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d67f65b7-0fea-40a3-ac78-795926025653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basic_qa_model = BasicQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7044ec84-f76a-4af4-93f5-a82579237868",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='What is the birthplace of the first author to win a Hugo Award for a translation?\\nAnswer: China'\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_model(question=\"What is the birthplace of the first author to win a Hugo Award for a translation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9961c89-914a-4736-8a01-1a3cd401821a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The modular design of DSPy starts to become apparent now. If you want to change the above to use chain of thought instead of regular predictions, you need only change `dspy.Predict` to `dspy.ChainOfThought`, and similarly for `dspy.ReAct`, `dspy.ProgramOfThought`, or a module you wrote yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9dac4-9495-448d-ad4b-38ab7260915b",
   "metadata": {},
   "source": [
    "### Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5200696-808f-4061-878c-26aa20701d58",
   "metadata": {},
   "source": [
    "The QA system we've defined so far is a zero-shot system. To change it into a few-shot system, we will rely on a DSPy optimizer (__teleprompter__). This will allow us to flexibly move between the zero-shot and few-shot formulations. The following code achieves this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdab9850-1d4d-4e04-b389-6ef55fd0b425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cache/compiler\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import LabeledFewShot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72045705-2f27-46fe-b7bc-801a7d2e9ae4",
   "metadata": {},
   "source": [
    "Here we instantiate a `LabeledFewShot` teleprompter that will add three demonstrations. These will be sampled randomly from the set of train examples we provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "befc8c18-fc64-4947-800d-6455dee90b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fewshot_teleprompter = LabeledFewShot(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b512d9-ec54-49ca-ab7a-900dc2eea6cc",
   "metadata": {},
   "source": [
    "And then we call `compile` on `basic_qa_model` as we defined it above. This returns a new module that we use like any other in DSPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b327b603-2943-4bcc-b498-1fbafafefd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basic_fewshot_qa_model = fewshot_teleprompter.compile(basic_qa_model, trainset=squad_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "921ec7f1-ed44-48a3-9c92-0e22911688f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='China'\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_fewshot_qa_model(question=\"What is the birthplace of the first author to win a Hugo Award for a translation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180cdaa-dd34-4e0d-8455-e394bfde9fcc",
   "metadata": {},
   "source": [
    "With `inspect_history`, we can see that prompts now contain demonstrations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9507fe6-1cc7-4c0a-8520-cc3812a07c27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "Answer: often between 1 and 5 words\n",
      "\n",
      "---\n",
      "\n",
      "Question: What group did Paul VI address in New York in 1965?\n",
      "Answer: United Nations\n",
      "\n",
      "---\n",
      "\n",
      "Question: What did Sander's study show in terms of black law students rankings?\n",
      "Answer: half of all black law students rank near the bottom of their class after the first year of law school\n",
      "\n",
      "---\n",
      "\n",
      "Question: What problems does linguistic anthropology bring linguistic methods to bear on?\n",
      "Answer: anthropological\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the birthplace of the first author to win a Hugo Award for a translation?\n",
      "Answer:\u001b[32m China\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba48440-4a65-41e8-b397-7b79f65fa0fe",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8734a-49a1-4093-a2fb-09bb7d2f2859",
   "metadata": {},
   "source": [
    "Our evaluation metric is a standard one for SQuAD and related tasks: exact match of the answer (EM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f052a79f-ad9f-4f3e-a195-809567e2eea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dspy.evaluate import answer_exact_match\n",
    "from dspy.evaluate.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7341a846-5158-4acf-8aa5-aca61ef40174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_exact_match(dspy.Example(answer=\"STAGE 2!\"), dspy.Prediction(answer=\"stage 2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640e5bf-823f-4cdb-92da-a28f5cea7760",
   "metadata": {},
   "source": [
    "In DSPy, `Evaluate` objects provide a uniform interface for running evaluations. Here are two for us to use in development. The first will evaluate on all of `dev_exs` and should provide a meaningful picture of how a system is doing. It could be expensive to use it a lot, though. The second is for debugging and is probably too small to give a reliable estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6a3b58b-c114-4316-b8d7-2d8049132c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_evaluater = Evaluate(\n",
    "    devset=dev_exs, # 200 examples\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    "    display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72fd504e-4684-445d-b0cc-363cd1685b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tiny_evaluater = Evaluate(\n",
    "    devset=dev_exs[: 15],\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    "    display_table=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d88a6c-55b6-4994-9413-1a2301c94903",
   "metadata": {},
   "source": [
    "Here is a tiny (debugging-oriented) evaluation of our few-shot QA sytem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f736065-39f2-4d76-a258-e852767dbb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 15  (13.3): 100%|██████████| 15/15 [00:05<00:00,  2.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fe60a th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_fe60a td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_fe60a_row0_col0, #T_fe60a_row0_col1, #T_fe60a_row0_col2, #T_fe60a_row0_col3, #T_fe60a_row1_col0, #T_fe60a_row1_col1, #T_fe60a_row1_col2, #T_fe60a_row1_col3, #T_fe60a_row2_col0, #T_fe60a_row2_col1, #T_fe60a_row2_col2, #T_fe60a_row2_col3, #T_fe60a_row3_col0, #T_fe60a_row3_col1, #T_fe60a_row3_col2, #T_fe60a_row3_col3, #T_fe60a_row4_col0, #T_fe60a_row4_col1, #T_fe60a_row4_col2, #T_fe60a_row4_col3 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fe60a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fe60a_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_fe60a_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_fe60a_level0_col2\" class=\"col_heading level0 col2\" >pred_answer</th>\n",
       "      <th id=\"T_fe60a_level0_col3\" class=\"col_heading level0 col3\" >answer_exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fe60a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fe60a_row0_col0\" class=\"data row0 col0\" >In 1517 who was Luther's bishop?</td>\n",
       "      <td id=\"T_fe60a_row0_col1\" class=\"data row0 col1\" >Albert of Mainz</td>\n",
       "      <td id=\"T_fe60a_row0_col2\" class=\"data row0 col2\" >Albert of Mainz</td>\n",
       "      <td id=\"T_fe60a_row0_col3\" class=\"data row0 col3\" >✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe60a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fe60a_row1_col0\" class=\"data row1 col0\" >When was the construction that changed the Rhine's Delta?</td>\n",
       "      <td id=\"T_fe60a_row1_col1\" class=\"data row1 col1\" >20th Century</td>\n",
       "      <td id=\"T_fe60a_row1_col2\" class=\"data row1 col2\" >13th century</td>\n",
       "      <td id=\"T_fe60a_row1_col3\" class=\"data row1 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe60a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fe60a_row2_col0\" class=\"data row2 col0\" >How many companies were registered in Warsaw in 2006?</td>\n",
       "      <td id=\"T_fe60a_row2_col1\" class=\"data row2 col1\" >304,016</td>\n",
       "      <td id=\"T_fe60a_row2_col2\" class=\"data row2 col2\" >over 100,000</td>\n",
       "      <td id=\"T_fe60a_row2_col3\" class=\"data row2 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe60a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fe60a_row3_col0\" class=\"data row3 col0\" >What is the CJEU's duty?</td>\n",
       "      <td id=\"T_fe60a_row3_col1\" class=\"data row3 col1\" >to \"ensure that in the interpretation and application of the Treaties the law is observed\"</td>\n",
       "      <td id=\"T_fe60a_row3_col2\" class=\"data row3 col2\" >interpret EU law</td>\n",
       "      <td id=\"T_fe60a_row3_col3\" class=\"data row3 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe60a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fe60a_row4_col0\" class=\"data row4 col0\" >What would a teacher do for someone who is cocky?</td>\n",
       "      <td id=\"T_fe60a_row4_col1\" class=\"data row4 col1\" >deflate</td>\n",
       "      <td id=\"T_fe60a_row4_col2\" class=\"data row4 col2\" >ignore them</td>\n",
       "      <td id=\"T_fe60a_row4_col3\" class=\"data row4 col3\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x31b2e5700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 10 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "13.33"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_evaluater(basic_fewshot_qa_model, metric=answer_exact_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1b41d-760c-4f28-8a2d-7f037b4f9d97",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4c8f4-a537-4d9b-9500-f881fceef1de",
   "metadata": {},
   "source": [
    "The final major component of our systems is retrieval. When we defined `rm`, we connected to a remote ColBERT index and retriever system that we can now use for search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfd114-96cf-468a-bac3-d3d39d6f3ca6",
   "metadata": {},
   "source": [
    "The basic `dspy.retrieve` method returns only passages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8f4cafc-f5db-41c0-9561-ecde61331666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = dspy.Retrieve(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "891fc391-c177-4da7-9332-ab20cdba3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = retriever(\"What is the birthplace of the first author to win a Hugo Award for a translation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abdac37b-b5fe-421c-826f-4fd699cb2e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ken Liu | Problem\" won the 2015 Hugo Award, the first translated novel in the award\\'s history to have won that honor. Ken Liu is currently writing his series \"The Dandelion Dynasty\" (edited by Joe Monti) for Saga Press. The first novel in the series, \"The Grace of Kings\", was a 2016 Nebula Award finalist. His official Star Wars novel called \"The Legends of Luke Skywalker\" was published October 31, 2017. Liu was born in 1976 in Lanzhou, China, and emigrated to the United States when he was 11 years old, initially living in Palo Alto, California, and later moving to Waterford, Connecticut.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages.passages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1f577-408c-4ede-9e27-65a24aafca5f",
   "metadata": {},
   "source": [
    "If we need passages with scores and other metadata, we can call `rm` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37f4ff3d-de41-4fc7-8943-6dfd894dd1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pid': 30459,\n",
       "  'prob': 1.0,\n",
       "  'rank': 1,\n",
       "  'score': 22.009323120117188,\n",
       "  'text': 'Ken Liu | Problem\" won the 2015 Hugo Award, the first translated novel in the award\\'s history to have won that honor. Ken Liu is currently writing his series \"The Dandelion Dynasty\" (edited by Joe Monti) for Saga Press. The first novel in the series, \"The Grace of Kings\", was a 2016 Nebula Award finalist. His official Star Wars novel called \"The Legends of Luke Skywalker\" was published October 31, 2017. Liu was born in 1976 in Lanzhou, China, and emigrated to the United States when he was 11 years old, initially living in Palo Alto, California, and later moving to Waterford, Connecticut.',\n",
       "  'long_text': 'Ken Liu | Problem\" won the 2015 Hugo Award, the first translated novel in the award\\'s history to have won that honor. Ken Liu is currently writing his series \"The Dandelion Dynasty\" (edited by Joe Monti) for Saga Press. The first novel in the series, \"The Grace of Kings\", was a 2016 Nebula Award finalist. His official Star Wars novel called \"The Legends of Luke Skywalker\" was published October 31, 2017. Liu was born in 1976 in Lanzhou, China, and emigrated to the United States when he was 11 years old, initially living in Palo Alto, California, and later moving to Waterford, Connecticut.'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm(\"What is the birthplace of the first author to win a Hugo Award for a translation?\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2017ee-1375-4251-a24f-7f792852ffac",
   "metadata": {},
   "source": [
    "## Few-shot OpenQA with context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c0997-e624-4fc0-824d-e13066978b0a",
   "metadata": {},
   "source": [
    "Let's build on the above core concepts to define a basic retrieval-augmented generation (RAG) program. This program solves the core task of few-shot OpenQA task and will serve as the basis for the homework questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714dd71-02aa-4e84-840f-807b4e501732",
   "metadata": {},
   "source": [
    "We begin with a signature that takes context into account but is otherwise just like `BasicQASignature` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcd35bc4-9bbc-4286-ad6b-d7474b51423e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextQASignature(dspy.Signature):\n",
    "    __doc__ = \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de481d-d4dd-42c3-99c1-7a112c7f521f",
   "metadata": {},
   "source": [
    "And here is a complete program/system for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f84481c9-6e6a-4331-a5e1-cf2b9e27b082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=1):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.Predict(ContextQASignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c4c1ddd-455e-4ad5-b1b1-bf1267364660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_model = RAG(num_passages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7fce28fa-aa8a-4cec-a07a-5365a5eb32df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    context=['Ken Liu | Problem\" won the 2015 Hugo Award, the first translated novel in the award\\'s history to have won that honor. Ken Liu is currently writing his series \"The Dandelion Dynasty\" (edited by Joe Monti) for Saga Press. The first novel in the series, \"The Grace of Kings\", was a 2016 Nebula Award finalist. His official Star Wars novel called \"The Legends of Luke Skywalker\" was published October 31, 2017. Liu was born in 1976 in Lanzhou, China, and emigrated to the United States when he was 11 years old, initially living in Palo Alto, California, and later moving to Waterford, Connecticut.', 'Vernor Vinge | Vernor Vinge Vernor Steffen Vinge (; born October 2, 1944) is an American science fiction author and retired professor. He taught mathematics and computer science at San Diego State University. He is the originator of the technological singularity concept and perhaps the first to present a fictional \"cyberspace\". He has won the Hugo Award for his novels and novellas \"A Fire Upon the Deep\" (1992), \"A Deepness in the Sky\" (1999), \"Rainbows End\" (2006), \"Fast Times at Fairmont High\" (2002), and \"The Cookie Monster\" (2004). Vinge published his first short story, \"Bookworm, Run!\", in the March 1966 issue of \"Analog', 'Translation | Awards annually present prizes for the best English-to-French and French-to-English literary translations. Other writers, among many who have made a name for themselves as literary translators, include Vasily Zhukovsky, Tadeusz Boy-Żeleński, Vladimir Nabokov, Jorge Luis Borges, Robert Stiller and Haruki Murakami. The first important translation in the West was that of the Septuagint, a collection of Jewish Scriptures translated into early Koine Greek in Alexandria between the 3rd and 1st centuries BCE. The dispersed Jews had forgotten their ancestral language and needed Greek versions (translations) of their Scriptures. Throughout the Middle Ages, Latin was the \"lingua franca\" of the western'],\n",
       "    answer='Lanzhou, China'\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_model(question=\"What is the birthplace of the first author to win a Hugo Award for a translation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873286e4-aaa7-4359-a5f2-04f8cbcceac8",
   "metadata": {},
   "source": [
    "An optional tiny evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25715550-2ba8-44bb-9a11-ca3bc3e0af56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 15  (60.0): 100%|██████████| 15/15 [00:09<00:00,  1.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b3f31 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b3f31 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b3f31_row0_col0, #T_b3f31_row0_col1, #T_b3f31_row0_col2, #T_b3f31_row0_col3, #T_b3f31_row0_col4, #T_b3f31_row1_col0, #T_b3f31_row1_col1, #T_b3f31_row1_col2, #T_b3f31_row1_col3, #T_b3f31_row1_col4, #T_b3f31_row2_col0, #T_b3f31_row2_col1, #T_b3f31_row2_col2, #T_b3f31_row2_col3, #T_b3f31_row2_col4, #T_b3f31_row3_col0, #T_b3f31_row3_col1, #T_b3f31_row3_col2, #T_b3f31_row3_col3, #T_b3f31_row3_col4, #T_b3f31_row4_col0, #T_b3f31_row4_col1, #T_b3f31_row4_col2, #T_b3f31_row4_col3, #T_b3f31_row4_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b3f31\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b3f31_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_b3f31_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_b3f31_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n",
       "      <th id=\"T_b3f31_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_b3f31_level0_col4\" class=\"col_heading level0 col4\" >answer_exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b3f31_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b3f31_row0_col0\" class=\"data row0 col0\" >In 1517 who was Luther's bishop?</td>\n",
       "      <td id=\"T_b3f31_row0_col1\" class=\"data row0 col1\" >Albert of Mainz</td>\n",
       "      <td id=\"T_b3f31_row0_col2\" class=\"data row0 col2\" >['Martin Luther | St. Peter\\'s Basilica in Rome. Roman Catholic theology stated that faith alone, whether fiduciary or dogmatic, cannot justify man; justification rather depends...</td>\n",
       "      <td id=\"T_b3f31_row0_col3\" class=\"data row0 col3\" >Albert of Mainz</td>\n",
       "      <td id=\"T_b3f31_row0_col4\" class=\"data row0 col4\" >✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3f31_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b3f31_row1_col0\" class=\"data row1 col0\" >When was the construction that changed the Rhine's Delta?</td>\n",
       "      <td id=\"T_b3f31_row1_col1\" class=\"data row1 col1\" >20th Century</td>\n",
       "      <td id=\"T_b3f31_row1_col2\" class=\"data row1 col2\" >['Rhine | characterized by the delta\\'s main arms, disconnected arms (Hollandse IJssel, Linge, Vecht, etc.) and smaller rivers and streams. Many rivers have been closed...</td>\n",
       "      <td id=\"T_b3f31_row1_col3\" class=\"data row1 col3\" >second half of the 20th Century</td>\n",
       "      <td id=\"T_b3f31_row1_col4\" class=\"data row1 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3f31_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b3f31_row2_col0\" class=\"data row2 col0\" >How many companies were registered in Warsaw in 2006?</td>\n",
       "      <td id=\"T_b3f31_row2_col1\" class=\"data row2 col1\" >304,016</td>\n",
       "      <td id=\"T_b3f31_row2_col2\" class=\"data row2 col2\" >['Warsaw | a \"major world city\") by the Globalization and World Cities (GaWC) Study Group and Network from Loughborough University, placing it on a par...</td>\n",
       "      <td id=\"T_b3f31_row2_col3\" class=\"data row2 col3\" >304,016</td>\n",
       "      <td id=\"T_b3f31_row2_col4\" class=\"data row2 col4\" >✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3f31_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b3f31_row3_col0\" class=\"data row3 col0\" >What is the CJEU's duty?</td>\n",
       "      <td id=\"T_b3f31_row3_col1\" class=\"data row3 col1\" >to \"ensure that in the interpretation and application of the Treaties the law is observed\"</td>\n",
       "      <td id=\"T_b3f31_row3_col2\" class=\"data row3 col2\" >['European Union law | judges for three years. While TEU article 19(3) says the Court of Justice is the ultimate court to interpret questions of...</td>\n",
       "      <td id=\"T_b3f31_row3_col3\" class=\"data row3 col3\" >\"ensure that in the interpretation and application of the Treaties the law is observed\"</td>\n",
       "      <td id=\"T_b3f31_row3_col4\" class=\"data row3 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3f31_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b3f31_row4_col0\" class=\"data row4 col0\" >What would a teacher do for someone who is cocky?</td>\n",
       "      <td id=\"T_b3f31_row4_col1\" class=\"data row4 col1\" >deflate</td>\n",
       "      <td id=\"T_b3f31_row4_col2\" class=\"data row4 col2\" >['Teacher | described the place of a teacher in learning as follows: \"The real bulk of learning takes place in self-study and problem solving with...</td>\n",
       "      <td id=\"T_b3f31_row4_col3\" class=\"data row4 col3\" >deflate</td>\n",
       "      <td id=\"T_b3f31_row4_col4\" class=\"data row4 col4\" >✔️ [True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x30bd47a30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 10 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_evaluater(rag_model, metric=answer_exact_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb28556-d901-4b6b-8a7d-93040203294d",
   "metadata": {},
   "source": [
    "## Question 1: Optimizing RAG [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f481081-3e16-4ccd-b379-c7e0c3011286",
   "metadata": {},
   "source": [
    "We used `RAG` above as a zero-shot system. We could turn it into a few-shot system by using `LabeledFewShot` as we did in [the teleprompting section](#Teleprompting) above, but this may actually be problematic: if we randomly sample demonstrations with retrieved passages, we might be instructing the model with a lot of cases where the context passage isn't helping (and may actually be actively misleading the model). \n",
    "\n",
    "What we'd like to do is select demonstrations where the model gets the answer correct and the context passage does contain the answer. To do this, we will use the DSPy `BootstrapFewShot` optimizer. There are two steps for this: (1) defining a metric and (2) running the optimizer.\n",
    "\n",
    "__Note__: The code for this question can be found in the DSPy tutorials, and you should feel free to make use of that code. The goal is to help you understand the design patterns and overall logic of optimizing DSPy programs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ae027-f51a-4607-822f-2a721acde73c",
   "metadata": {},
   "source": [
    "__Task 1__: Complete `validate_context_and_answer` according to the specification in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad09f428-1a68-4cef-9e7a-7c6faf9244a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    \"\"\"Return True if `example.answer` matches `pred.answer` according\n",
    "    to `dspy.evaluate.answer_exact_match` and `pred.context` contains\n",
    "    `example.answer` according to `dspy.evaluate.answer_passage_match`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    example: dspy.Example \n",
    "        with attributes `answer` and `context`\n",
    "    pred: dspy.Example \n",
    "        with attributes `answer` and `context`\n",
    "    trace : None (included for dspy internal compatibility)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if predicted answer matches example answer\n",
    "    answer_match = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    \n",
    "    # Check if context contains the example answer\n",
    "    context_match = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    \n",
    "    # Return True only if both conditions are met\n",
    "    return answer_match and context_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0918c5f-2864-4856-adcf-ccb94aca6108",
   "metadata": {},
   "source": [
    "A test you can use to check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddbca59c-7c11-4e4b-bec3-18a2ecde563b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_validate_context_and_answer(func):\n",
    "    examples = [\n",
    "        (\n",
    "            dspy.Example(question=\"Q1\", answer=\"B\"),\n",
    "            dspy.Prediction(question=\"Q1\", context=\"A B C\", answer=\"B\"),\n",
    "            True\n",
    "        ),\n",
    "        # Context doesn't contain answer, but predicted answer is correct.\n",
    "        (\n",
    "            dspy.Example(question=\"Q1\", answer=\"D\"),\n",
    "            dspy.Prediction(question=\"Q1\", context=\"A B C\", answer=\"D\"),\n",
    "            False\n",
    "        ),\n",
    "        # Context contains answer, but predicted answer is not correct.\n",
    "        (\n",
    "            dspy.Example(question=\"Q1\", answer=\"C\"),\n",
    "            dspy.Prediction(question=\"Q1\", context=\"A B C\", answer=\"D\"),\n",
    "            False\n",
    "        )\n",
    "    ]\n",
    "    errcount = 0\n",
    "    for ex, pred, result in examples:\n",
    "        predicted = func(ex, pred, trace=None)\n",
    "        if predicted != result:\n",
    "            errcount += 1\n",
    "            print(f\"Error for `{func.__name__}`: \"\n",
    "                  f\"Expected inputs\\n\\t{ex}\\n\\t{pred} to return {result}.\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors detected for `{func.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38aa3141-ae8f-4bc6-a26e-64d108537612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected for `validate_context_and_answer`\n"
     ]
    }
   ],
   "source": [
    "test_validate_context_and_answer(validate_context_and_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091ca79-ccbe-4429-bfe7-c4f1de118adf",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Task 2__: Complete `bootstrap_optimize` according to the specification in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "129e7e92-033e-4f38-b309-94dac2b66d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "def bootstrap_optimize(model):\n",
    "    \"\"\"Use `BootstrapFewShot` to optimize `model`, with the metric set\n",
    "    to `validate_context_and_answer` as defined above and default\n",
    "    values for all other keyword arguments to `BootstrapFewShot`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: dspy.Module\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dspy.Module, the optimized version of `model`\n",
    "\n",
    "    \"\"\"\n",
    "    bootstrap = BootstrapFewShot(\n",
    "        metric=validate_context_and_answer\n",
    "    )\n",
    "    return bootstrap.compile(model, trainset=squad_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26af22-6d7d-4456-9e30-8f80f5b0b401",
   "metadata": {},
   "source": [
    "A test you can use to check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30dd9f56-31cc-4e45-8bc2-0fe56c3806a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_bootstrap_optimize(func):\n",
    "    model = RAG()\n",
    "    compiled = func(model)\n",
    "    if not hasattr(compiled, \"_compiled\") or not compiled._compiled:\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "               \"The return value is not a compiled program.\")\n",
    "        return None\n",
    "    state = compiled.dump_state()\n",
    "    if not state['generate_answer']['demos']:\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "               \"The compiled program has no `demos`.\")\n",
    "        return None\n",
    "    print(f\"No errors detected for `{func.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "733c1c31-9c68-4d2a-a1c3-4eed0e1caa6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/87599 [00:03<13:39:17,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 7 examples in round 0.\n",
      "[('retrieve', <dspy.retrieve.retrieve.Retrieve object at 0x30bda7730>), ('generate_answer', Predict(ContextQASignature(context, question -> answer\n",
      "    instructions='Answer questions with short factoid answers.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'desc': 'may contain relevant facts', '__dspy_field_type': 'input', 'prefix': 'Context:'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'often between 1 and 5 words', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")))]\n",
      "No errors detected for `bootstrap_optimize`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_bootstrap_optimize(bootstrap_optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971ffd0-a563-4d56-a896-0735a63ae92f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 2: Multi-passage summarization [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e4d85-05d7-4bc2-beaf-2434d4fe41da",
   "metadata": {},
   "source": [
    "The `dspy.Retrieve` layer in our `RAG` retrieves `k` passages, where `k` is under the control of the user. One hypothesis one might have is that it would be good to summarize these passages before using them as evidence. This seems especially likely to help in scenarios where the question can be answered only by synthesizing information across documents – it might be too much to ask the language model to do both synthesizing and answering in a single step.\n",
    "\n",
    "The current question maps out a basic strategy for summarization. The heart of it is a new signature called `SummarizeSignature`. This can be used on its own with a simple `dspy.Predict` call, and we'll incorporate it into a RAG program in the next question.\n",
    "\n",
    "For this question, though, your task is just to complete `SummarizeSignature`. The requirements are as follows:\n",
    "\n",
    "1. A `__doc__` value that gives an instruction that seems to work well. You can decide what to say here.\n",
    "2. A `dspy.InputField` named `context`. You can decide whether to use the `desc` parameter.\n",
    "3. A `dspy.OutputField` named `summary`. You can decide whether to use the `desc` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c080a6-d2e1-4d7a-ac60-5ef3a6931ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SummarizeSignature(dspy.Signature):\n",
    "    pass\n",
    "    ##### YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6091f20-1cae-4e04-bd50-929271ae6a18",
   "metadata": {},
   "source": [
    "Here's a simple test that just checks for the required pieces in a basic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c971a-2dcd-4344-afa0-8e52938ca4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_SummarizeSignature(sigclass):\n",
    "    fields = sigclass.fields\n",
    "    expected_fieldnames = ['context', 'summary']\n",
    "    fieldnames = sorted(fields)\n",
    "    errcount = 0\n",
    "    if expected_fieldnames != fieldnames:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{sigclass.__name__}`: \"\n",
    "              f\"Expected fieldnames {expected_fieldnames}, got {fieldnames}.\")\n",
    "    if not sigclass.__doc__:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{sigclass.__name__}`: No docstring specified.\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors detected for `{sigclass.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec39e4d-b3ca-4355-9695-c9f61a24f3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_SummarizeSignature(SummarizeSignature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ee5d0-2bd6-4cb0-873c-21f963a78555",
   "metadata": {},
   "source": [
    "Here is the simplest way to use `SummarizeSignature`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8129a3-90fb-4810-b18b-84e75525dd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer = dspy.Predict(SummarizeSignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfdb34-07d8-4488-a1ed-1b4788a2a119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizer(context=retriever(\"Where is Guarani spoken?\").passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7a44f-c50d-4674-9fbb-fcf216222e3a",
   "metadata": {},
   "source": [
    "## Question 3: Summarizing RAG [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812e189-2778-442f-81b1-577c303445a8",
   "metadata": {},
   "source": [
    "Your task for this question is to modify `RAG` as defined above so that the retrieved passages are summarized before being passed to `generate_answer`. \n",
    "\n",
    "Here is the `RAG` system copied from above with the class name changed to the one we will use for this new system. Your task is to add the summarization step. This should be very straightforward given the modular design that DSPy supports and encourages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa956847-d6ff-40cf-bf02-9043863e548e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SummarizingRAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        # Please name your summarization later `summarize` so that we\n",
    "        # can check for its presence.\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        ##### YOUR CODE HERE\n",
    "\n",
    "\n",
    "        self.generate_answer = dspy.Predict(ContextQASignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        ##### YOUR CODE HERE\n",
    "\n",
    "\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b65d8-14ca-4dbe-a815-a0d00940b0b4",
   "metadata": {},
   "source": [
    "A simple test for this design spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f063d-1735-4bf3-9337-e9728a5b7800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_SummarizingRAG(classname):\n",
    "    model = classname(num_passages=3)\n",
    "    errcount = 0\n",
    "    if not hasattr(model, \"summarize\"):\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{classname.__name__}`: \"\n",
    "              f\"Expected a layer called 'summarize'\")\n",
    "    context = model.retrieve(\"What are some foods?\").passages\n",
    "    pred = model(\"What are some foods?\")\n",
    "    if context == pred.context:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{classname.__name__}`: \"\n",
    "              \"The model seems to be using raw retrieved contexts \"\n",
    "              \"for predictions rather than summarizing them.\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors detected for `{classname.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d38c4-540c-4473-b464-0061b5b8a09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_SummarizingRAG(SummarizingRAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3b6ad-b960-4b8f-8137-a9b90953b2fc",
   "metadata": {},
   "source": [
    "Model usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e03778-5a7f-4119-ac62-f4965bfe9a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizing_rag_model = SummarizingRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077370b-156f-4738-948b-86d832b2ebbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarizing_rag_model(question=\"What is the birthplace of the first author to win a Hugo Award for a translation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebc3e9-7baf-4e1c-81a0-8c1e3588a882",
   "metadata": {},
   "source": [
    "Note: if you decide to use `BootstrapFewShot` on this, be sure not to use the metric we defined above, which requires that the passage embeds the correct answer as a substring. Now that we are summarizing, this is unlikely to hold, even if the answers are good ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19655d70-007c-4a41-9f3b-e20df9c5169b",
   "metadata": {},
   "source": [
    "## Question 4: Your original system [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d268f-70e1-4325-a2ff-7361d73788b9",
   "metadata": {},
   "source": [
    "This question asks you to design your own few-shot OpenQA system. All of the code above can be used and modified for this, and the requirement is just that you try something new that goes beyond what we've done so far. \n",
    "\n",
    "Terms for the bake-off:\n",
    "\n",
    "* You can make free use of SQuAD and other publicly available data.\n",
    "\n",
    "* The LM must be an autoregressive language model. No trained QA components can be used. This includes general purpose LMs that have been fine-tuned for QA. (We have obviously waded into some vague territory here. The spirit of this is to make use of frozen, general-purpose models. We welcome questions about exactly how this is defined, since it could be instructive to explore this.)\n",
    "\n",
    "Here are some ideas for the original system:\n",
    "\n",
    "* We have relied almost entirely on `dspy.Predict`. Drop-in replacements include `dspy.ChainOfThought` and `dspy.ReAct`.\n",
    "\n",
    "* We have used only one retriever. DSPy supports other retrieval mechanisms, including retrieval using [You.com](https://you.com/).\n",
    "\n",
    "* DSPy includes additional optimizers. Two that are worth trying are `SignatureOptimizer` for automatic prompt exploration and `BootstrapFewShotWithRandomSearch`, which combines `LabeledFewShot` and `BootstrapFewShot`,\n",
    "\n",
    "* Our one-step summarization procedure from Question 3 doesn't change the query to the retriever. We might want it to change as we gather evidence. This is a common design principle for multi-hop OpenQA systems.\n",
    "\n",
    "__Original system instructions__:\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557f3c3-ee72-480e-9d99-9095372f99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
    "#   1) Textual description of your system.\n",
    "#   2) The code for your original system.\n",
    "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
    "\n",
    "# START COMMENT: Enter your system description in this cell.\n",
    "\n",
    "def foo(s):\n",
    "    return True\n",
    "\n",
    "# STOP COMMENT: Please do not remove this comment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b39c60-7494-46a6-b450-42b7e9fe3aad",
   "metadata": {},
   "source": [
    "## Question 5: Bakeoff entry [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff871c1-cc38-4e2f-af38-45b3619e8329",
   "metadata": {},
   "source": [
    "For the bake-off, you simply need to be able to run your system on the file \n",
    "\n",
    "```data/openqa/cs224u-openqa-test-unlabeled.txt```\n",
    "\n",
    "The following code should download it for you if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca87f81-556b-46eb-904f-a3df70fdacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "if not os.path.exists(os.path.join(\"data\", \"openqa\", \"cs224u-openqa-test-unlabeled.txt\")):\n",
    "    os.makedirs(os.path.join('data', 'openqa'), exist_ok=True)\n",
    "    wget.download('https://web.stanford.edu/class/cs224u/data/cs224u-openqa-test-unlabeled.txt', out='data/openqa/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0024b-9af7-4e3b-930e-1e7603d4d85c",
   "metadata": {},
   "source": [
    "If the above fails, you can just download https://web.stanford.edu/class/cs224u/data/cs224u-openqa-test-unlabeled.txt and place it in `data/openqa`.\n",
    "\n",
    "This file contains only questions. The starter code below will help you structure this. It writes a file \"cs224u-openqa-bakeoff-entry.json\" to the current directory. That file should be uploaded as-is. Please do not change its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b0000-5bc0-4657-91e4-5a6e87f2f899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "\n",
    "def create_bakeoff_submission(model):\n",
    "    \"\"\"\"\n",
    "    The argument `model` is a `dspy.Module`. The return value of its\n",
    "    `forward` method must have an `answer` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(\"data\", \"openqa\", \"cs224u-openqa-test-unlabeled.txt\")\n",
    "\n",
    "    # This should become a mapping from questions (str) to response\n",
    "    # dicts from your system.\n",
    "    gens = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        questions = f.read().splitlines()\n",
    "\n",
    "    # Here we loop over the questions, run the system `model`, and\n",
    "    # store its `answer` value as the prediction:\n",
    "    for question in tqdm.tqdm(questions):\n",
    "        gens[question] = model(question=question).answer\n",
    "\n",
    "    # Quick tests we advise you to run:\n",
    "    # 1. Make sure `gens` is a dict with the questions as the keys:\n",
    "    assert all(question in gens for q in questions)\n",
    "    # 2. Make sure the values are str:\n",
    "    assert all(isinstance(d, str) for d in gens.values())\n",
    "\n",
    "    # And finally the output file:\n",
    "    with open(\"cs224u-openqa-bakeoff-entry.json\", \"wt\") as f:\n",
    "        json.dump(gens, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f32a8-547e-4a2c-8283-44adf69657ed",
   "metadata": {},
   "source": [
    "Here's what it looks like to evaluate our first program, `basic_qa_model`, on the bakeoff data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20e9ae-bb82-4dff-896f-aad7f150177d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_bakeoff_submission(basic_qa_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
